package agent

import (
	"context"
	"fmt"
	"sync"

	"github.com/aquasecurity/starboard/pkg/apis/aquasecurity/v1alpha1"
	"github.com/aquasecurity/starboard/pkg/kube"
	"gitlab.com/gitlab-org/cluster-integration/gitlab-agent/v15/internal/module/modagent"
	"gitlab.com/gitlab-org/cluster-integration/gitlab-agent/v15/internal/tool/logz"
	"go.uber.org/zap"
	appsv1 "k8s.io/api/apps/v1"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/wait"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

const maxParallel int = 10 // scan jobs batch size

var workloadKinds = []kube.Kind{
	kube.KindPod,
	kube.KindReplicaSet,
	kube.KindReplicationController,
	kube.KindStatefulSet,
	kube.KindDaemonSet,
	kube.KindCronJob,
	kube.KindJob,
}

type VulnerabilityReportScanner interface {
	Scan(context.Context, kube.ObjectRef) ([]v1alpha1.VulnerabilityReport, error)
}

type scanJob struct {
	vulnerabilityReportScanner VulnerabilityReportScanner
	core                       client.Client
	log                        *zap.Logger
	api                        modagent.Api
	agentID                    int64
	targetNamespaces           []string
}

func (s *scanJob) Run(ctx context.Context) {
	if err := s.scan(ctx, s.targetNamespaces); err != nil {
		s.log.Error("error running vulnerability scan", logz.Error(err))
	}
}

type uuidCollection struct {
	uuids []string
	mux   sync.Mutex
}

func (u *uuidCollection) Append(uuids []string) {
	u.mux.Lock()
	u.uuids = append(u.uuids, uuids...)
	u.mux.Unlock()
}

func (u *uuidCollection) Items() []string {
	u.mux.Lock()
	defer u.mux.Unlock()
	return u.uuids
}

func (s *scanJob) scan(ctx context.Context, targetNamespaces []string) error {
	if len(targetNamespaces) == 0 {
		var err error
		targetNamespaces, err = s.listAllNamespaces(ctx)
		if err != nil {
			return fmt.Errorf("error listing namespaces: %w", err)
		}
	}

	workloads := s.enumerateScannableWorkloads(ctx, targetNamespaces)
	if len(workloads) == 0 {
		return nil
	}

	var wg wait.Group
	limit := make(chan struct{}, maxParallel)
	var allUuids uuidCollection
	reporter := NewReporter(s.log, s.api)

	for i := range workloads {
		workload := workloads[i]

		wg.Start(func() {
			limit <- struct{}{}

			defer func() { <-limit }()

			s.log.Sugar().Debugf("Scanning workload: %s/%s/%s", workload.Namespace, workload.Kind, workload.Name)
			// A single workload can have multiple containers.
			// We receive one report for each container.
			reports, err := s.vulnerabilityReportScanner.Scan(ctx, workload)
			if err != nil {
				s.log.Error("Failed to perform vulnerability scan on workload", logz.Error(err))
				return
			}

			for j := range reports {
				// Avoid implicit memory aliasing
				report := &reports[j]

				logger := s.log.With(
					logz.ReportName(report.ObjectMeta.Name),
				)

				payloads, err := Convert(report, s.agentID)
				if err != nil {
					logger.Error("Error processing vulnerability report", logz.Error(err))
					return
				}

				var uuids []string
				uuids, err = reporter.Transmit(ctx, payloads)
				if err != nil {
					logger.Error("Error transmitting vulnerability reports", logz.Error(err))
					return
				}

				allUuids.Append(uuids)
			}

		})
	}

	wg.Wait()

	s.log.Info("Resolving no longer detected vulnerabilities in GitLab")

	err := reporter.ResolveVulnerabilities(ctx, allUuids.Items())
	if err != nil {
		return fmt.Errorf("Error resolving vulnerabilities: %w", err)
	}

	return nil
}

func (s *scanJob) listAllNamespaces(ctx context.Context) ([]string, error) {
	list := new(corev1.NamespaceList)
	err := s.core.List(ctx, list)
	if err != nil {
		return nil, fmt.Errorf("error requesting namespace list: %w", err)
	}

	namespaces := make([]string, len(list.Items))
	for i, item := range list.Items {
		namespaces[i] = item.Name
	}

	return namespaces, nil
}

func (s *scanJob) enumerateScannableWorkloads(ctx context.Context, namespaces []string) []kube.ObjectRef {
	s.log.Debug("Enumerating scannable workloads")

	workloads := make([]kube.ObjectRef, 0)
	for _, namespace := range namespaces {
		for _, kind := range workloadKinds {
			list := listObject(kind)
			if err := s.core.List(ctx, list, client.InNamespace(namespace)); err != nil {
				s.log.Sugar().Errorf("Could not discover workloads of type %q. Workloads of this type will not be scanned", kind, logz.Error(err), logz.Kind(string(kind)))
				continue
			}

			_ = meta.EachListItem(list, func(obj runtime.Object) error {
				accessor, err := meta.Accessor(obj)
				if err != nil {
					s.log.Error("could not access object metadata", logz.Error(err))
					return nil
				}

				// Ignore pods which are controlled by built-in Kubernetes workloads
				if kind == kube.KindPod {
					controller := metav1.GetControllerOf(accessor)
					if kube.IsBuiltInWorkload(controller) {
						s.log.Sugar().Debugf("skipping pod %q controlled by built-in workload: %s/%s", accessor.GetName(), controller.Kind, controller.Name)
						return nil
					}
				}

				workload := kube.ObjectRef{
					Name:      accessor.GetName(),
					Namespace: accessor.GetNamespace(),
					Kind:      kind,
				}

				workloads = append(workloads, workload)
				return nil
			})
		}
	}

	s.log.Sugar().Debugf("Found %d workloads to scan", len(workloads))
	return workloads
}

func listObject(kind kube.Kind) client.ObjectList {
	var obj client.ObjectList

	// We only check for workload types
	// nolint:exhaustive
	switch kind {
	case kube.KindPod:
		obj = &corev1.PodList{}
	case kube.KindReplicaSet:
		obj = &appsv1.ReplicaSetList{}
	case kube.KindReplicationController:
		obj = &corev1.ReplicationControllerList{}
	case kube.KindStatefulSet:
		obj = &appsv1.StatefulSetList{}
	case kube.KindDaemonSet:
		obj = &appsv1.DaemonSetList{}
	case kube.KindCronJob:
		obj = &batchv1.CronJobList{}
	case kube.KindJob:
		obj = &batchv1.JobList{}
	default:
		// Should never happen. Verify this with tests.
		panic(fmt.Sprintf("Given kind %q is not a workload type", kind))
	}

	return obj
}
